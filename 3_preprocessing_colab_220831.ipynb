{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk1GsL_Owzr-"
      },
      "source": [
        "# Text preprocessing\n",
        "\n",
        "220831, wygo\n",
        "\n",
        "- 1) Crawling&Gathering paper/report/blog/SNS/AIhub etc... on HTML/PDF format\n",
        "- 2) Parsing HTML/PDF to txt\n",
        "- 3) Cleansing noisy text\n",
        "- 4) Normalizing noisy text\n",
        "- 5) Making bert format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tika\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBd0p0f-w7Ae",
        "outputId": "10439410-1f47-4e9f-836a-bd351df36490"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2022.6.15)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=aeb8f4d38df064faecb4393c3d29404781ca7b72139c967ae0c219eed4eb8816\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0tUPw6-wzsA",
        "outputId": "6eee28a3-73ac-47d9-aee5-9bb44643ffe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture_hyperparameter_tuning'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 72 (delta 3), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n"
          ]
        }
      ],
      "source": [
        "# download data\n",
        "!git clone https://github.com/airobotlab/lecture_hyperparameter_tuning.git\n",
        "!mv lecture_hyperparameter_tuning/data ./\n",
        "!rm -rf lecture_hyperparameter_tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKMYg0UwzsB"
      },
      "source": [
        "## 1) Crawling&Gathering paper/report/blog/SNS/AIhub etc... on HTML/PDF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "z9KBBNE1wzsC"
      },
      "outputs": [],
      "source": [
        "## import\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import chain\n",
        "from tika import parser as pdf_parser\n",
        "import pprint\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "def enter_space_normalizer(text):\n",
        "    text = re.sub(\"\\n\\n+\",  \"\\n\\n\", text,  0,  re.I | re.S)\n",
        "    text = re.sub(\" +\",   \" \", text,  0,  re.I | re.S)\n",
        "    return text.strip()\n",
        "\n",
        "print_json = pprint.PrettyPrinter(indent=4).pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "code_folding": [
          0
        ],
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrpIWDYxwzsC",
        "outputId": "fd99cb2e-5989-4487-bd0d-43d8a6aabf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:00, 48.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####  크롤링 시작  ##################################################\n",
            "## input_path : data/4\n",
            "## input_html : data/4/Java_Zero_day_vulnerability_exploited_in_the_Wild.html\n",
            "## output_txt : data/4/filtered_text.txt\n",
            "####  크롤링 완료  ##################################################\n",
            "Java Zero-day vulnerability exploited in the Wild\n",
            "\n",
            "Really a bad weekend for Internet users.\n",
            "Three previously unknown critical zero-day vulnerabilities were revealed in Adobe's Flash Player over the weekend, thanks to Hacking team data Breach in which 400GB of internal data were leaked over the Internet.\n",
            "\n",
            "Now, a new zero-day vulnerability has been reported in Oracle's Java that is reportedly being exploited in the wild by hackers to target government armed forces.\n",
            "\n",
            "Cybercriminals are actively exploiting the Java-based zero-day flaw in an attempt to target U.S. defense agencies and members of NATO, Trend Micro security researchers warned in a blog post published Sunday.\n",
            "\n",
            " var adpushup = adpushup || {}; adpushup.que = adpushup.que || []; adpushup.que.push(function() { adpushup.triggerAd(\"967ecfad-bf6b-429e-9a39-9770c8b7d188\"); });\n",
            "\n",
            "According to researchers, the vulnerability affects only the latest version of Java, version 1.8.0.45.\n",
            "Though the older Java versions, Java 1.6 and 1.7 are not at all affected by this zero-day exploit.\n",
            "\n",
            "So far, there isn't many details disclosed about the Java zero-day bug, considering a patch is yet to be released by Oracle.\n",
            "Although hackers are exploiting the zero-day flaw through drive-by-downloads attack.\n",
            "\n",
            "Java Zero-Day Exploit in the Wild\n",
            "\n",
            "Cyber criminals are using email messages to spread the malicious links hosting the Java zero-day exploit.\n",
            "Once clicked, the exploit code delivers a basic Trojan dropper, TROJ_DROPPR.CXC, that drops a payload called TSPY_FAKEMS.C into the \"/login user\" folder.\n",
            "\n",
            "From login user folder, the malware executes an arbitrary code on the default Java settings thus compromising the security of the system.\n",
            "\n",
            "Researchers have also unearthed an attack that leverages a three-year-old Windows vulnerability identified as CVE-2012-015, which Microsoft addressed in Bulletin MS12-027 three years ago.\n",
            "\n",
            "Operation Pawn Storm APT Group Behind Java 0_day Exploit\n",
            "\n",
            "The advanced persistent threat (APT) group Operation Pawn Storm are thought to be responsible for the Java zero-day exploit attacking the member of NATO and the US defense organization, but the security firm did not disclose the names where the attack was sighted.\n",
            "\n",
            "Pawn Storm, a group of hackers specialized in cyber-espionage operation, has been active since 2007 and has also been known by different names, including APT28, Sednit, Fancy Bear, and Tsar Team.\n",
            "\n",
            "Are You Vulnerable to New Java Zero-Day Exploit?\n",
            "\n",
            "Oracle developers are working with Trend Micro to develop a fix to patch the issue.\n",
            "Until the patch is rolled out, users are advised to disable Java temporarily in their browser.\n",
            "####  크롤링 시작  ##################################################\n",
            "## input_path : data/1\n",
            "## input_html : data/1/What_does_the_Poetry_with_Citadel_trojan__.html\n",
            "## output_txt : data/1/filtered_text.txt\n",
            "####  크롤링 완료  ##################################################\n",
            "What does the Poetry with Citadel trojan ?\n",
            "\n",
            "Recently we published an article on the attacks against Japanese banks using a new variant of the popular Zeus, one of the most prolific malware of recent history, security experts in fact have detected various versions of the popular malicious code that hit also mobile and social networking platforms.\n",
            "\n",
            "Due its flexibility the malware has been re-engineered several times by cyber criminals that adapted its structure to specific purposes and context, leaving unchanged its core capabilities of stealing banking credentials of victims.\n",
            "\n",
            " var adpushup = adpushup || {}; adpushup.que = adpushup.que || []; adpushup.que.push(function() { adpushup.triggerAd(\"967ecfad-bf6b-429e-9a39-9770c8b7d188\"); });\n",
            "\n",
            "Zeus has been a huge success in the criminal circles especially for the sales model, as malware as service, implemented by its authors on many underground sites, let's remind for example the Citadel Trojan one of the most popular on the crimeware market.\n",
            "Fortunately its author, known as Aquabox, has been banned from a large online forum that sells malware and other services to cyber criminals, but many security firms consider Citadel Trojan still very active threat that continues to infect many machines all over the world.\n",
            "\n",
            "Security experts from McAfee Labs are sure that the agent will remain active for a long time, it also indicate that some groups of hackers are staring to use the malware for other purposes such as the cyber espionage.\n",
            "McAfee Global Threat Intelligence report indicates the \"Poetry Group\" is one of the most active in this sense, the collective compromised 27 Japanese government offices across three distinct campaigns and targeted around 43 government offices in Poland.\n",
            "The group was very aggressive in October 2012 when it conducted more than a half-dozen campaigns infecting victims in Poland, Denmark, Sweden, Spain, Netherlands, Estonia, Czech Republic, Switzerland, and Japan, compromising more than 1,000 victims worldwide.\n",
            "\n",
            "The researchers from McAfee Labs were able to pinpoint the regions \"and identify targets and victims spanning more than a half-dozen campaigns\", the highest infection rate were registered in Denmark, Poland, Spain and Japan.Curiously Japan is one of the most targeted countries by cyber espionage campaign, in many cases we have spoken about cyber attacks malware based that hit industry and government offices of the state.\n",
            "\n",
            "The victims located in Poland appears to be all government offices and the exerts discovered that attackers conducted a targeted campaign on specific targets across the country from December 2012 to January 2013.\n",
            "\n",
            "Nice the way the hackers used to \"identify\" their works, they in fact added strings of poetry in the malware binary, Ryan Sherstobitoff, a McAfee researcher declared: \"We've found them making political statements against the groups they are targeting,\"\n",
            "\n",
            "McAfee analysts detected more that 300 unique Citadel Trojan samples, each of them included its sequence of poetry strings that aren't automatically generated , the specialists suspect that Poetry Group may be a byproduct of a for-hire data-gathering operation for a private clientele.\n",
            "\n",
            "In the fight against malware such as Citadel it is fundamental a layered approach that was able to detect the cyber threats and also any suspect behavior within target networks.\n",
            "In many cases such malware are demonstrated to be able to elude common antivirus systems and their behavioral detection mode.\n",
            "\n",
            "Unfortunately these agents are able to remain silently for a long period infiltrating internal systems and remaining undetected in the target networks for long time.\n",
            "\n",
            "The adaptation of the Citadel malware for other uses is a scaring signal due the capabilities of the malicious agent, the Trojan make possible remote control of victims and is able to steal any kind of information, not only banking credentials from victims.\n",
            "Sherstobitoff added: \"If they wanted to penetrate the entire network of a financial institution or some other organization, they could,\"\n",
            "\n",
            "Waiting for further updates on the operations of the Poetry Group lets keep update antivirus software and any other application that runs on our systems, because as Sherstobitoff added \"These attacks result from not taking patch management seriously,\".\n",
            "####  크롤링 시작  ##################################################\n",
            "## input_path : data/3\n",
            "## input_html : data/3/Faulty_Patch_for_Oracle_WebLogic_Flaw_Opens_Updated_Servers_to_Hackers_Again.html\n",
            "## output_txt : data/3/filtered_text.txt\n",
            "####  크롤링 완료  ##################################################\n",
            "Faulty Patch for Oracle WebLogic Flaw Opens Updated Servers to Hackers Again\n",
            "\n",
            "Earlier this month, Oracle patched a highly critical Java deserialization remote code execution vulnerability in its WebLogic Server component of Fusion Middleware that could allow attackers to easily gain complete control of a vulnerable server.\n",
            "\n",
            "However, a security researcher, who operates through the Twitter handle @pyn3rd and claims to be part of the Alibaba security team, has now found a way using which attackers can bypass the security patch and exploit the WebLogic vulnerability once again.\n",
            "\n",
            " var adpushup = adpushup || {}; adpushup.que = adpushup.que || []; adpushup.que.push(function() { adpushup.triggerAd(\"967ecfad-bf6b-429e-9a39-9770c8b7d188\"); });\n",
            "\n",
            "WebLogic Server acts as a middle layer between the front end user interface and the backend database of a multi-tier enterprise application.\n",
            "It provides a complete set of services for all components and handles details of the application behavior automatically.\n",
            "\n",
            "Initially discovered in November last year by Liao Xinxi of NSFOCUS security team, the Oracle WebLogic Server flaw (CVE-2018-2628) can be exploited with network access over TCP port 7001.\n",
            "\n",
            "If exploited successfully, the flaw could allow a remote attacker to completely take over a vulnerable Oracle WebLogic Server.\n",
            "The vulnerability affects versions 10.3.6.0, 12.1.3.0, 12.2.1.2 and 12.2.1.3.\n",
            "\n",
            "Since a proof-of-concept (PoC) exploit for the original Oracle WebLogic Server vulnerability has already been made public on Github and someone has just bypassed the patch as well, your up-to-date services are again at risk of being hacked.\n",
            "\n",
            " var adpushup = adpushup || {}; adpushup.que = adpushup.que || []; adpushup.que.push(function() { adpushup.triggerAd(\"8c2d7f94-a9c5-43b2-83a4-cdcf711ae05e\"); });\n",
            "\n",
            "Although @pyn3rd has only released a short GIF (video) as a proof-of-concept (PoC) instead of releasing full bypass code or any technical details, it would hardly take a few hours or days for skilled hackers to figure out a way to achieve same.\n",
            "\n",
            "Currently, it is unclear when Oracle would release a new security update to address this issue that has re-opened CVE-2018-2628 flaw.\n",
            "\n",
            "In order to be at least one-step safer, it is still advisable to install April patch update released by Oracle, if you haven't yet because attackers have already started scanning the Internet for vulnerable WebLogic servers.\n",
            "####  크롤링 시작  ##################################################\n",
            "## input_path : data/2\n",
            "## input_html : data/2/Latest_Report_Uncovers_Supply_Chain_Attacks_by_North_Korean_Hackers.html\n",
            "## output_txt : data/2/filtered_text.txt\n",
            "####  크롤링 완료  ##################################################\n",
            "Latest Report Uncovers Supply Chain Attacks by North Korean Hackers\n",
            "\n",
            "Lazarus Group, the advanced persistent threat (APT) group attributed to the North Korean government, has been observed waging two separate supply chain attack campaigns as a means to gain a foothold into corporate networks and target a wide range of downstream entities.\n",
            "\n",
            "The latest intelligence-gathering operation involved the use of MATA malware framework as well as backdoors dubbed BLINDINGCAN and COPPERHEDGE to attack the defense industry, an IT asset monitoring solution vendor based in Latvia, and a think tank located in South Korea, according to a new Q3 2021 APT Trends report published by Kaspersky.\n",
            "\n",
            "In one instance, the supply-chain attack originated from an infection chain that stemmed from legitimate South Korean security software running a malicious payload, leading to the deployment of the BLINDINGCAN and COPPERHEDGE malware on the think tank's network in June 2021.\n",
            "The other attack on the Latvian company in May is an \"atypical victim\" for Lazarus, the researchers said.\n",
            "\n",
            "It's not clear if Lazarus tampered with the IT vendor's software to distribute the implants or if the group abused the access to the company's network to breach other customers.\n",
            "The Russian cybersecurity firm is tracking the campaign under the DeathNote cluster.\n",
            "\n",
            "That's not all.\n",
            "In what appears to be a different cyber-espionage campaign, the adversary has also been spotted leveraging the multi-platform MATA malware framework to perform an array of malicious activities on infected machines.\n",
            "\"The actor delivered a Trojanized version of an application known to be used by their victim of choice, representing a known characteristic of Lazarus,\" the researchers noted.\n",
            "\n",
            "According to previous findings by Kaspersky, the MATA campaign is capable of striking Windows, Linux, and macOS operating systems, with the attack infrastructure enabling the adversary to carry out a multi-staged infection chain that culminates in the loading of additional plugins, which allow access to a wealth of information including files stored on the device, extract sensitive database information as well as inject arbitrary DLLs.\n",
            "\n",
            " var adpushup = adpushup || {}; adpushup.que = adpushup.que || []; adpushup.que.push(function() { adpushup.triggerAd(\"8c2d7f94-a9c5-43b2-83a4-cdcf711ae05e\"); });\n",
            "\n",
            "Beyond Lazarus, a Chinese-speaking APT threat actor, suspected to be HoneyMyte, was found adopting the same tactic, wherein a fingerprint scanner software installer package was modified to install the PlugX backdoor on a distribution server belonging to a government agency in an unnamed country in South Asia.\n",
            "Kaspersky referred to the supply-chain incident as \"SmudgeX.\"\n",
            "\n",
            "The development comes as cyber attacks aimed at the IT supply chain have emerged as a top concern in the wake of the 2020 SolarWinds intrusion, highlighting the need to adopt strict account security practices and take preventive measures to protect enterprise environments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## 실행\n",
        "verbose = True\n",
        "output_txt_path='filtered_text.txt'\n",
        "file_format = '*.html'\n",
        "# file_format = '/*.pdf'\n",
        "json_format = '*.json'\n",
        "html_src = 'data'\n",
        "\n",
        "# for idx, input_path in tqdm(enumerate(os.listdir(html_src)[:1])):\n",
        "for idx, input_path in tqdm(enumerate(os.listdir(html_src))):    \n",
        "#     print(idx, input_path)\n",
        "\n",
        "    input_path = os.path.join(html_src, input_path)\n",
        "    output_txt_path='filtered_text.txt'\n",
        "\n",
        "    ###########################################################\n",
        "    # 입력 파일 체크, 저장path 설정\n",
        "    if not os.path.exists(input_path):\n",
        "        print('Folder not exist! : %s'%(input_path))\n",
        "        assert 'Folder not exists! : %s'%(input_path)\n",
        "    output_txt_path = os.path.join(input_path, output_txt_path)\n",
        "    \n",
        "    ###########################################################\n",
        "    ## 0) inspection\n",
        "    # 0.1) html or pdf check\n",
        "    # 폴더 안의 html 파일만 검색\n",
        "    file_path = glob.glob(os.path.join(input_path, file_format))\n",
        "    # html이 안들어 있는 경우 패스\n",
        "    if len(file_path) == 0:\n",
        "        print('no have file! - %s' % (os.path.join(input_path, file_format)))\n",
        "    else:\n",
        "        file_path = file_path[0]  # list 에서 html만 선택\n",
        "    file_name = file_path.split('/')[-1].split('.')[0]  # folder의 html의 이름\n",
        "    input_file_path = file_path\n",
        "   \n",
        "    ###########################################################\n",
        "    ## 1) file check\n",
        "    # 1.1) pdf file check\n",
        "    if verbose:\n",
        "        print('####  크롤링 시작  %s'%('#'*50))\n",
        "        print('## input_path : %s'%(input_path))\n",
        "        print('## input_html : %s'%(input_file_path))\n",
        "        print('## output_txt : %s'%(output_txt_path))\n",
        "    \n",
        "    # 파일이 없으면 멈추자\n",
        "    if not os.path.exists(input_file_path):\n",
        "        print('1.1) File not exist! : %s'%(input_file_path))\n",
        "        assert '1.1) File not exists! : %s'%(input_file_path)\n",
        "\n",
        "    ###########################################################\n",
        "    ## 2) parsing pdf or html to txt\n",
        "    result = {}  # 모든 결과 저장\n",
        "    try:\n",
        "        if 'pdf' in file_format:\n",
        "            print('format: PDF')\n",
        "            xml = pdf_parser.from_file(input_file_path, xmlContent=True)\n",
        "            list_page = xml['content'].split('<div class=\"page\">')\n",
        "\n",
        "            string = ''\n",
        "            for page in list_page:\n",
        "                string += page\n",
        "            string = string.strip()\n",
        "            result['raw_data'] = string\n",
        "\n",
        "        elif 'html' in file_format:\n",
        "#             print('format: HTML')\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "                html = f.read()\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                string = soup.text.strip()\n",
        "                result['raw_data'] = string\n",
        "        else:\n",
        "            assert 'Not pdf or html'\n",
        "\n",
        "    except Exception as error:\n",
        "        print('2) load error', error)\n",
        "    \n",
        "    ###########################################################\n",
        "    ## 3) cutting\n",
        "    # 3.1) 본문 뒷쪽 자르기\n",
        "    string = string.replace(u'\\ue802', r' ').replace(u'\\ue804', r' ')  # 이상한 특수문자 제거\n",
        "    pattern_end = 'Found this article interesting?'\n",
        "    string_end = string.split(pattern_end)[0].strip()\n",
        "    result['string_end'] = string_end\n",
        "    \n",
        "    # 3.2) 제목과 앞쪽 자르기\n",
        "    pattern_start = '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
        "    title = string_end.split(pattern_start)[0].strip()\n",
        "    start_string_end = string_end.split(pattern_start)[1].strip()\n",
        "    result['title'] = title\n",
        "    result['start_string_end'] = start_string_end\n",
        "#     print('Title : %s'%title)    \n",
        "    \n",
        "    # 3.3) 첫줄인 날짜를 없앤 후 문단 단위로 쪼갬 \n",
        "    pattern_content = '\\n'       \n",
        "    content = start_string_end.split(pattern_content)[1:] # 제목, 날짜 날림\n",
        "    content = '\\n'.join(content)\n",
        "    content_split = content.split(pattern_content)  # 문단 단위로 쪼갬    \n",
        "    result['content_split'] = content_split\n",
        "    \n",
        "    # 3.4) AI입력형태로 변환, 1line 1 sentence, 문단 간 빈줄 하나\n",
        "    sentence_normalized = [sent_tokenize(sentences) for sentences in content_split]\n",
        "    # 1 line 1 sentence, 문단이 다르면 빈줄 하나 추가\n",
        "    tmp = []\n",
        "    for list_sentence in sentence_normalized:\n",
        "        tmp.append('\\n'.join(list_sentence))\n",
        "    tmp_string = '\\n\\n'.join(tmp)\n",
        "    final_string = enter_space_normalizer(tmp_string)\n",
        "    \n",
        "    # 3.5) 제목이 있으면 맨 위에 제목, 다음 빈줄\n",
        "    if len(title) > 5:\n",
        "        final_string = title + '\\n\\n' + final_string\n",
        "    \n",
        "    result['final_string'] = final_string\n",
        "\n",
        "    ###########################################################\n",
        "    ## 5) check\n",
        "    if len(final_string) < 20:\n",
        "        print('텍스트 크롤링 실패: %s\\n%s'%(folder_name, final_string))\n",
        "\n",
        "    if verbose:\n",
        "        print('####  크롤링 완료  %s'%('#'*50))\n",
        "        print(result['final_string'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "code_folding": [
          1
        ],
        "id": "nnYgd55wwzsE"
      },
      "outputs": [],
      "source": [
        "## 함수화\n",
        "def parsing_html(input_path, output_txt_path='filtered_text.txt', file_format = '*.html', verbose=False):\n",
        "    ###########################################################\n",
        "    # 입력 파일 체크, 저장path 설정\n",
        "    if not os.path.exists(input_path):\n",
        "        print('Folder not exist! : %s'%(input_path))\n",
        "        assert 'Folder not exists! : %s'%(input_path)\n",
        "    output_txt_path = os.path.join(input_path, output_txt_path)\n",
        "    \n",
        "    ###########################################################\n",
        "    ## 0) inspection\n",
        "    # 0.1) html or pdf check\n",
        "    # 폴더 안의 html 파일만 검색\n",
        "    file_path = glob.glob(os.path.join(input_path, file_format))\n",
        "    # html이 안들어 있는 경우 패스\n",
        "    if len(file_path) == 0:\n",
        "        print('no have file! - %s' % (os.path.join(input_path, file_format)))\n",
        "    else:\n",
        "        file_path = file_path[0]  # list 에서 html만 선택\n",
        "    file_name = file_path.split('/')[-1].split('.')[0]  # folder의 html의 이름\n",
        "    input_file_path = file_path\n",
        "   \n",
        "    ###########################################################\n",
        "    ## 1) file check\n",
        "    # 1.1) pdf file check\n",
        "    if verbose:\n",
        "        print('####  크롤링 시작  %s'%('#'*50))\n",
        "        print('## input_path : %s'%(input_path))\n",
        "        print('## input_html : %s'%(input_file_path))\n",
        "        print('## output_txt : %s'%(output_txt_path))\n",
        "    \n",
        "    # 파일이 없으면 멈추자\n",
        "    if not os.path.exists(input_file_path):\n",
        "        print('1.1) File not exist! : %s'%(input_file_path))\n",
        "        assert '1.1) File not exists! : %s'%(input_file_path)\n",
        "\n",
        "    ###########################################################\n",
        "    ## 2) parsing pdf or html to txt\n",
        "    result = {}  # 모든 결과 저장\n",
        "    try:\n",
        "        if 'pdf' in file_format:\n",
        "            print('format: PDF')\n",
        "            xml = pdf_parser.from_file(input_file_path, xmlContent=True)\n",
        "            list_page = xml['content'].split('<div class=\"page\">')\n",
        "\n",
        "            string = ''\n",
        "            for page in list_page:\n",
        "                string += page\n",
        "            string = string.strip()\n",
        "            result['raw_data'] = string\n",
        "\n",
        "        elif 'html' in file_format:\n",
        "#             print('format: HTML')\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "                html = f.read()\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                string = soup.text.strip()\n",
        "                result['raw_data'] = string\n",
        "        else:\n",
        "            assert 'Not pdf or html'\n",
        "\n",
        "    except Exception as error:\n",
        "        print('2) load error', error)\n",
        "    \n",
        "    ###########################################################\n",
        "    ## 3) cutting\n",
        "    # 3.1) 본문 뒷쪽 자르기\n",
        "    string = string.replace(u'\\ue802', r' ').replace(u'\\ue804', r' ')  # 이상한 특수문자 제거\n",
        "    pattern_end = 'Found this article interesting?'\n",
        "    string_end = string.split(pattern_end)[0].strip()\n",
        "    result['string_end'] = string_end\n",
        "    \n",
        "    # 3.2) 제목과 앞쪽 자르기\n",
        "    pattern_start = '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
        "    title = string_end.split(pattern_start)[0].strip()\n",
        "    start_string_end = string_end.split(pattern_start)[1].strip()\n",
        "    result['title'] = title\n",
        "    result['start_string_end'] = start_string_end\n",
        "#     print('Title : %s'%title)    \n",
        "    \n",
        "    # 3.3) 첫줄인 날짜를 없앤 후 문단 단위로 쪼갬 \n",
        "    pattern_content = '\\n'       \n",
        "    content = start_string_end.split(pattern_content)[1:] # 제목, 날짜 날림\n",
        "    content = '\\n'.join(content)\n",
        "    content_split = content.split(pattern_content)  # 문단 단위로 쪼갬    \n",
        "    result['content_split'] = content_split\n",
        "    \n",
        "    # 3.4) AI입력형태로 변환, 1line 1 sentence, 문단 간 빈줄 하나\n",
        "    sentence_normalized = [sent_tokenize(sentences) for sentences in content_split]\n",
        "    # 1 line 1 sentence, 문단이 다르면 빈줄 하나 추가\n",
        "    tmp = []\n",
        "    for list_sentence in sentence_normalized:\n",
        "        tmp.append('\\n'.join(list_sentence))\n",
        "    tmp_string = '\\n\\n'.join(tmp)\n",
        "    final_string = enter_space_normalizer(tmp_string)\n",
        "    \n",
        "    # 3.5) 제목이 있으면 맨 위에 제목, 다음 빈줄\n",
        "    if len(title) > 5:\n",
        "        final_string = title + '\\n\\n' + final_string\n",
        "    \n",
        "    result['final_string'] = final_string\n",
        "\n",
        "    ###########################################################\n",
        "    ## 5) check\n",
        "    if len(final_string) < 20:\n",
        "        print('텍스트 크롤링 실패: %s\\n%s'%(folder_name, final_string))\n",
        "\n",
        "    if verbose:\n",
        "        print('####  크롤링 완료  %s'%('#'*50))\n",
        "        print(result['final_string'])\n",
        "        \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yaVZ_G0CwzsE"
      },
      "outputs": [],
      "source": [
        "result  = parsing_html(input_path='./data/1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bzhACkwzsF"
      },
      "source": [
        "### check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nik2SaodwzsF",
        "outputId": "88bb9a01-9781-4043-cec8-0b32c7517d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What does the Poetry with Citadel trojan ?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "February 23, 2013Anonymous\n",
            "Recently we published an article on the attacks against Japanese banks using a new variant of the popular Zeus, one of the most prolific malware of recent history, security experts in fact have detected various versions of the popular malicious code that hit also mobile and social networking platforms.\n",
            "Due its flexibility the malware has been re-engineered several times by cyber criminals that adapted its stru\n"
          ]
        }
      ],
      "source": [
        "print(result['raw_data'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8-FdPHkwzsG",
        "outputId": "52408c0e-c7e0-40cf-deb4-26d40058e83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What does the Poetry with Citadel trojan ?\n",
            "\n",
            "Recently we published an article on the attacks against Japanese banks using a new variant of the popular Zeus, one of the most prolific malware of recent history, security experts in fact have detected various versions of the popular malicious code that hit also mobile and social networking platforms.\n",
            "\n",
            "Due its flexibility the malware has been re-engineered several times by cyber criminals that adapted its structure to specific purposes and context, le\n"
          ]
        }
      ],
      "source": [
        "print(result['final_string'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "id": "jrl7rFgiwzsG"
      },
      "source": [
        "## 3) Cleansing noisy text & 4) Normalizing noisy text\n",
        "## Unicode 삭제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "code_folding": [
          0,
          3
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CJi4UMJwzsG",
        "outputId": "c36e43d2-079b-4d3c-ffe9-dce4f6db22f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-31 05:08:39--  https://jehyunlee.github.io/2022/08/27/Python-DS-111-textprocfn/unicode_dict.zip\n",
            "Resolving jehyunlee.github.io (jehyunlee.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jehyunlee.github.io (jehyunlee.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1070 (1.0K) [application/zip]\n",
            "Saving to: ‘unicode_dict.zip’\n",
            "\n",
            "unicode_dict.zip    100%[===================>]   1.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-08-31 05:08:39 (34.3 MB/s) - ‘unicode_dict.zip’ saved [1070/1070]\n",
            "\n",
            "Archive:  ./data/unicode_dict.zip\n",
            "  inflating: unicode_dict.json       \n",
            "What does the Poetry with Citadel trojan ?\n",
            "\n",
            "Recently we published an article on the attacks against Japanese banks using a new variant of the popular Zeus, one of the most prolific malware of recent history, security experts in fact have detected various versions of the popular malicious code that hit also mobile and social networking platforms.\n",
            "\n",
            "Due its flexibility the malware has been re-engineered several times by cyber criminals that adapted its structure to specific purposes and context, le\n"
          ]
        }
      ],
      "source": [
        "# unicode 딕셔너리 다운\n",
        "import json\n",
        "\n",
        "if not os.path.isfile('./data/unicode_dict.json'):\n",
        "    URL_unicode = 'https://jehyunlee.github.io/2022/08/27/Python-DS-111-textprocfn/unicode_dict.zip'    \n",
        "    \n",
        "    !wget https://jehyunlee.github.io/2022/08/27/Python-DS-111-textprocfn/unicode_dict.zip\n",
        "    !mv unicode_dict.zip ./data/\n",
        "    !unzip ./data/unicode_dict.zip\n",
        "    !mv unicode_dict.json ./data/\n",
        "    os.remove('./data/unicode_dict.zip')\n",
        "\n",
        "with open(\"./data/unicode_dict.json\", encoding='utf-8') as f:\n",
        "    dict_unicode = json.load(f)\n",
        "\n",
        "# unicode filtering\n",
        "def get_ufiltered(text):\n",
        "    text_ufiltered = \"\"\n",
        "    for i, c in enumerate(text):\n",
        "        if dict_unicode.get(c):\n",
        "            ufiltered = dict_unicode[c]\n",
        "        else:\n",
        "            ufiltered = c\n",
        "        text_ufiltered = text_ufiltered + ufiltered\n",
        "    return text_ufiltered\n",
        "\n",
        "# 함수 실행\n",
        "result['unicode_filtered'] = get_ufiltered(result['final_string'])\n",
        "print(result['unicode_filtered'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2PoV8ZjwzsG"
      },
      "source": [
        "## 5) Making bert format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "bLpdQb6UwzsH"
      },
      "outputs": [],
      "source": [
        "## config\n",
        "html_src = 'data'\n",
        "\n",
        "html_bert_out= html_src + '_bert'\n",
        "output_txt_path = 'filtered_text.txt'\n",
        "\n",
        "file_format = '*.html'\n",
        "# file_format = '/*.pdf'\n",
        "\n",
        "start_symbol_and_solr_id = '<<doc>>_data_'\n",
        "\n",
        "max_tokens = 50\n",
        "min_tokens = 1\n",
        "min_characters = 2  # 2 이상만 담자, '.', ':' 이런것들은 제끼자\n",
        "\n",
        "# 특수문자 제거 : ['‑', '€', 'медии', 'カジ旅', '—', 'чуждите', '²', 'كازينو', 'ñ', '\\xad', 'ä', '|', '\\ufeff', '—\\u2009', '–', '®', '£', 'ü', '‐', 'è', 'в', 'БГ', '•', '…', 'А', 'é', '™', 'ï']\n",
        "remove_pattern = '[^0-9a-zA-Z ,.\\\"\\'()`~!@#$%^&*()-_=+\\[\\]:;{}<>?/\\n\\t]+'  # 이상한 문자들 제거 패턴\n",
        "black_list_pattern = []\n",
        "remove_words = ['\\xa0', '\\xad']\n",
        "stopword_list = []\n",
        "# stopword_list = ['.setvalue(']  # txt 파일 저장 에러내는 string 제거\n",
        "# stopword_list = ['APT40 relies heavily on web shells for an initial foothold into an organization.'.lower(),\n",
        "#                  'Depending on placement, a web shell can provide continued access to victims\\' environments, re-infect victim systems, and facilitate lateral movement.'.lower(),\n",
        "#                 'Web shells are heavily relied on for nearly all stages of the attack lifecycle.'.lower(),\n",
        "#                 'APT40 uses many methods for lateral movement throughout an environment, including custom scripts, web shells, a variety of tunnelers, as well as Remote Desktop Protocol (RDP).'.lower(),\n",
        "#                 'APT40 primarily uses backdoors, including web shells, to maintain presence within a victim environment.'.lower(),\n",
        "#                 'APT40 strongly favors web shells for maintaining presence, especially publicly available tools.'.lower()] # ['.setvalue(']\n",
        "\n",
        "if os.path.isdir(html_bert_out):  \n",
        "    shutil.rmtree(html_bert_out)\n",
        "else:\n",
        "    os.mkdir(html_bert_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTu8KnDwwzsH",
        "outputId": "246198d0-80c1-444c-890f-2542d79c518b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 96.73it/s]\n"
          ]
        }
      ],
      "source": [
        "## root 폴더 내 전체에 대한 일괄 파싱\n",
        "data_root = 'data'\n",
        "\n",
        "file_list = os.listdir(data_root)\n",
        "all_txt = []\n",
        "\n",
        "for folder_path in tqdm(file_list):\n",
        "    input_folder_path = os.path.join(data_root, folder_path)\n",
        "    if not os.path.isdir(input_folder_path):\n",
        "        continue\n",
        "    result = parsing_html(input_path=input_folder_path)\n",
        "    all_txt.append(result['final_string'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC4YcePKwzsH",
        "outputId": "afe1b1e1-07e4-4127-b1c0-ca6d5261060c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['||']\n"
          ]
        }
      ],
      "source": [
        "## 특수문자 체크, 이거에 안들어가면 재끼자\n",
        "all_special_string = []\n",
        "for document in all_txt:\n",
        "    special_string = re.findall(remove_pattern, document)\n",
        "    all_special_string.extend(special_string)\n",
        "\n",
        "remove_special_token = list(set(all_special_string))\n",
        "print(remove_special_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "XegSUew8wzsH"
      },
      "outputs": [],
      "source": [
        "## bert 입력 만들기\n",
        "for idx, document in enumerate(all_txt):\n",
        "    with open(os.path.join(html_bert_out, 'bert.txt'), 'a', encoding='utf-8') as f_bert:\n",
        "        bert_txt  = start_symbol_and_solr_id  + str(idx) + '\\n'\n",
        "        bert_txt += document  + '\\n\\n\\n'\n",
        "        f_bert.write(bert_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "84YY-ui-wzsH",
        "outputId": "db14fe2d-03ba-4f01-aa1f-7ca6753c47e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!! - 4개 문서, 65개 문장, 평균 26 단어/문장, 최대/최소 : 66 / 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 문장 통계\n",
        "# 모든 txt의 문장만 list로 집합시키자\n",
        "document_cnt, total_nmt_sentences = 0, []\n",
        "for document_string in all_txt:\n",
        "    document_cnt += 1\n",
        "    for document_sentence in document_string.split('\\n'):\n",
        "        if document_sentence !='': \n",
        "            total_nmt_sentences.append(document_sentence)\n",
        "            \n",
        "num_word_of_sentence_list = [len(tmp.split()) for tmp in total_nmt_sentences]\n",
        "print('done!! - %d개 문서, %d개 문장, 평균 %d 단어/문장, 최대/최소 : %d / %d' % (document_cnt, len(num_word_of_sentence_list)-document_cnt, (sum(num_word_of_sentence_list)/len(num_word_of_sentence_list)), max(num_word_of_sentence_list), min(num_word_of_sentence_list)))\n",
        "plt.hist(num_word_of_sentence_list, bins=max_tokens);\n",
        "beep = lambda x: os.system(\"echo -n '\\a';sleep 0.3;\" * x);beep(3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zEuvOtJ7wzsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HGG9DEWFwzsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JGqXf8UTwzsI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "3_preprocessing_colab_220831.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}